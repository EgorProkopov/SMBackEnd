{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from LookGenerator.networks.losses import WassersteinLoss, GradientPenalty, FineGANLoss\n",
    "from LookGenerator.datasets.encoder_decoder_datasets import EncoderDecoderDataset\n",
    "from LookGenerator.networks.fine_gan import *\n",
    "from LookGenerator.networks.clothes_feature_extractor import ClothAutoencoder\n",
    "from LookGenerator.networks.trainer import WGANGPTrainer\n",
    "from LookGenerator.networks_training.utils import check_path_and_creat\n",
    "from LookGenerator.networks.utils import get_num_digits, save_model, load_model\n",
    "import LookGenerator.datasets.transforms as custom_transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:58.740646100Z",
     "start_time": "2023-06-02T11:57:57.497981900Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "transform_input = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "transform_real = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    custom_transforms.MinMaxScale()\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:58.743636900Z",
     "start_time": "2023-06-02T11:57:58.741649200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "batch_size_train = 32\n",
    "pin_memory = True\n",
    "num_workers = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:58.758097700Z",
     "start_time": "2023-06-02T11:57:58.744145500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_dataset = EncoderDecoderDataset(\n",
    "    image_dir=r\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoderNew\\train\",\n",
    "    transform_human=transform_input,\n",
    "    transform_clothes=transform_input,\n",
    "    transform_human_restored=transform_real\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:58.776486700Z",
     "start_time": "2023-06-02T11:57:58.759092600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size_train, shuffle=True, pin_memory=pin_memory, num_workers=num_workers\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:58.837074600Z",
     "start_time": "2023-06-02T11:57:58.776486700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:58.841791700Z",
     "start_time": "2023-06-02T11:57:58.838078700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# def _epoch_string(epoch, epoch_num):\n",
    "#     num_digits_epoch_num = get_num_digits(epoch_num)\n",
    "#     num_digits_epoch = get_num_digits(epoch)\n",
    "#\n",
    "#     epoch_string = \"0\"*(num_digits_epoch_num - num_digits_epoch) + str(epoch)\n",
    "#     return epoch_string\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:58.849609400Z",
     "start_time": "2023-06-02T11:57:58.842787100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# def fit(model, criterion, gradient_penalty, train_dl, device, epochs, g_lr, d_lr,\n",
    "#         save_directory_generator, save_directory_discriminator, save_step=1):\n",
    "#     model[\"discriminator\"].train()\n",
    "#     model[\"generator\"].train()\n",
    "#     torch.cuda.empty_cache()\n",
    "#\n",
    "#     # Losses & scores\n",
    "#     losses_g = []\n",
    "#     losses_d = []\n",
    "#     real_scores = []\n",
    "#     fake_scores = []\n",
    "#\n",
    "#     # Create optimizers\n",
    "#     optimizer = {\n",
    "#         \"discriminator\": torch.optim.Adam(model[\"discriminator\"].parameters(),\n",
    "#                                           lr=d_lr, betas=(0.5, 0.999)),\n",
    "#         \"generator\": torch.optim.Adam(model[\"generator\"].parameters(),\n",
    "#                                       lr=g_lr, betas=(0.5, 0.999))\n",
    "#     }\n",
    "#\n",
    "#     for epoch in range(epochs):\n",
    "#         loss_d_per_epoch = []\n",
    "#         loss_g_per_epoch = []\n",
    "#         real_score_per_epoch = []\n",
    "#         fake_score_per_epoch = []\n",
    "#         model['discriminator'] = model['discriminator'].to(device)\n",
    "#         model['generator'] = model['generator'].to(device)\n",
    "#         for iteration, (input_images, real_images) in enumerate(tqdm(train_dl), 0):\n",
    "#             input_images = input_images.to(device)\n",
    "#             real_images = real_images.to(device)\n",
    "#             # Train discriminator\n",
    "#             # Clear discriminator gradients\n",
    "#             optimizer[\"discriminator\"].zero_grad()\n",
    "#\n",
    "#             real_images = real_images.to(device)\n",
    "#\n",
    "#             # Pass real images through discriminator\n",
    "#             real_preds = model[\"discriminator\"](real_images)\n",
    "#             real_targets = torch.ones(real_images.shape[0], 1, device=device)\n",
    "#             real_loss = criterion[\"discriminator\"](real_preds, real_targets)\n",
    "#             cur_real_score = torch.mean(real_preds).item()\n",
    "#\n",
    "#             # Generate fake images\n",
    "#             fake_images = model[\"generator\"](input_images)\n",
    "#\n",
    "#             # Pass fake images through discriminator\n",
    "#             fake_targets = torch.ones(fake_images.shape[0], 1, device=device)\n",
    "#             fake_preds = model[\"discriminator\"](fake_images)\n",
    "#             fake_loss = criterion[\"discriminator\"](fake_preds, fake_targets)\n",
    "#             cur_fake_score = torch.mean(fake_preds).item()\n",
    "#             gp = gradient_penalty(model[\"discriminator\"], real_images, fake_images, device)\n",
    "#\n",
    "#             real_score_per_epoch.append(cur_real_score)\n",
    "#             fake_score_per_epoch.append(cur_fake_score)\n",
    "#\n",
    "#             # Update discriminator weights\n",
    "#             loss_d = real_loss + fake_loss + 10.0 * gp\n",
    "#             loss_d.backward()\n",
    "#             optimizer[\"discriminator\"].step()\n",
    "#             loss_d_per_epoch.append(loss_d.item())\n",
    "#\n",
    "#             # Train generator\n",
    "#             if iteration % 5 == 0:\n",
    "#             # Clear generator gradients\n",
    "#                 optimizer[\"generator\"].zero_grad()\n",
    "#\n",
    "#                 # Generate fake images\n",
    "#                 fake_images = model[\"generator\"](input_images)\n",
    "#\n",
    "#                 # Try to fool the discriminator\n",
    "#                 preds = model[\"discriminator\"](fake_images)\n",
    "#                 targets = torch.ones(real_images.shape[0], 1, device=device)\n",
    "#                 loss_g = criterion[\"generator\"](preds, targets, fake_images, real_images)\n",
    "#\n",
    "#                 # Update generator weights\n",
    "#                 loss_g.backward()\n",
    "#                 optimizer[\"generator\"].step()\n",
    "#                 loss_g_per_epoch.append(loss_g.item())\n",
    "#\n",
    "#                 losses_g.append(np.mean(loss_g_per_epoch))\n",
    "#\n",
    "#         # Record losses & scores\n",
    "#         losses_d.append(np.mean(loss_d_per_epoch))\n",
    "#         real_scores.append(np.mean(real_score_per_epoch))\n",
    "#         fake_scores.append(np.mean(fake_score_per_epoch))\n",
    "#\n",
    "#         # Log losses & scores (last batch)\n",
    "#         print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
    "#             epoch+1, epochs,\n",
    "#             losses_g[-1], losses_d[-1], real_scores[-1], fake_scores[-1])\n",
    "#         )\n",
    "#\n",
    "#         if (epoch + 1) % save_step == 0:\n",
    "#             save_model(\n",
    "#                 model[\"discriminator\"].to('cpu'),\n",
    "#                 path=f\"{save_directory_discriminator}\\\\discriminator_epoch_{_epoch_string(epoch, epochs)}.pt\"\n",
    "#             )\n",
    "#             save_model(\n",
    "#                 model[\"generator\"].to('cpu'),\n",
    "#                 path=f\"{save_directory_generator}\\\\generator_epoch_{_epoch_string(epoch, epochs)}.pt\"\n",
    "#             )\n",
    "#\n",
    "#     return losses_g, losses_d, real_scores, fake_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:58.856294900Z",
     "start_time": "2023-06-02T11:57:58.849609400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory_generator=r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\encoderGAN\\weights\\gen\\testBaseParam\"\n",
    "save_directory_discriminator=r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\encoderGAN\\weights\\discr\\testBaseParm\"\n",
    "check_path_and_creat(save_directory_generator)\n",
    "check_path_and_creat(save_directory_discriminator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:58.863984700Z",
     "start_time": "2023-06-02T11:57:58.856294900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "clothes_feature_extractor = ClothAutoencoder(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    features=(8, 16, 32, 64),\n",
    "    latent_dim_size=128,\n",
    "    encoder_activation_func=nn.LeakyReLU(),\n",
    "    decoder_activation_func=nn.ReLU()\n",
    ")\n",
    "clothes_feature_extractor = load_model(clothes_feature_extractor, r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\autoDegradation\\weights\\testClothes_L1Loss_4features\\epoch_39.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:58.902112900Z",
     "start_time": "2023-06-02T11:57:58.863984700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DenisovDmitrii\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DenisovDmitrii\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "generator = EncoderDecoderGenerator(clothes_feature_extractor=clothes_feature_extractor, in_channels=3, out_channels=3, final_activation_func=nn.Sigmoid())\n",
    "discriminator = Discriminator()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "wasserstein_criterion = WassersteinLoss()\n",
    "gradient_penalty = GradientPenalty(discriminator, device=device)\n",
    "\n",
    "criterion_generator = FineGANLoss(\n",
    "    adversarial_criterion=wasserstein_criterion, adv_loss_weight=0.25,\n",
    "    l1_criterion=True, l1_loss_weight=4,\n",
    "    perceptual=True, perceptual_loss_weight=1, device=device\n",
    ")\n",
    "criterion_discriminator = WassersteinLoss()\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:59.684817600Z",
     "start_time": "2023-06-02T11:57:58.873515800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "optimizer_generator = torch.optim.Adam(params=generator.parameters(), lr=0.001)\n",
    "optimizer_discriminator = torch.optim.Adam(params=discriminator.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:59.686812800Z",
     "start_time": "2023-06-02T11:57:59.684287800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"discriminator\": discriminator.to(device),\n",
    "    \"generator\": generator.to(device)\n",
    "}\n",
    "\n",
    "criterion = {\n",
    "    \"discriminator\": criterion_discriminator.to(device),\n",
    "    \"generator\": criterion_generator.to(device)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:59.704154100Z",
     "start_time": "2023-06-02T11:57:59.687816700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "trainer = WGANGPTrainer(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    optimizer_generator=optimizer_generator,\n",
    "    optimizer_discriminator=optimizer_discriminator,\n",
    "    criterion_generator=criterion_generator,\n",
    "    criterion_discriminator=criterion_discriminator,\n",
    "    gradient_penalty=gradient_penalty,\n",
    "    gp_weight=0.2,\n",
    "    save_step=1,\n",
    "    save_directory_discriminator=save_directory_discriminator,\n",
    "    save_directory_generator=save_directory_generator,\n",
    "    device=device,\n",
    "    verbose=True\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:59.708312200Z",
     "start_time": "2023-06-02T11:57:59.706153500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# history = fit(model=model,\n",
    "#               criterion=criterion,\n",
    "#               gradient_penalty=gradient_penalty,\n",
    "#               train_dl=train_dataloader,\n",
    "#               device=device,\n",
    "#               epochs=10,\n",
    "#               g_lr=0.0001,\n",
    "#               d_lr=0.0001,\n",
    "#               save_directory_generator=save_directory_generator,\n",
    "#               save_directory_discriminator=save_directory_discriminator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T00:03:41.695059Z",
     "end_time": "2023-04-11T01:02:58.180449Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "epoch_num = 20"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:57:59.741341Z",
     "start_time": "2023-06-02T11:57:59.709306900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time 02-06-2023 14:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/364 [00:00<?, ?it/s]C:\\Users\\DenisovDmitrii\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "  0%|          | 0/364 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "GradientPenalty.forward() missing 2 required positional arguments: 'real_image' and 'device'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepoch_num\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch_num\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\python\\SMBackEnd\\LookGenerator\\networks\\trainer.py:498\u001B[0m, in \u001B[0;36mWGANGPTrainer.train\u001B[1;34m(self, train_dataloader, epoch_num)\u001B[0m\n\u001B[0;32m    493\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstart time\u001B[39m\u001B[38;5;124m\"\u001B[39m, start\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m    495\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch_num):\n\u001B[0;32m    496\u001B[0m     \u001B[38;5;66;03m# Train epoch\u001B[39;00m\n\u001B[1;32m--> 498\u001B[0m     loss_real, loss_fake, loss_d, loss_g \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    500\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose:\n\u001B[0;32m    501\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch_num\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, discriminator loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss_d\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.5f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mC:\\python\\SMBackEnd\\LookGenerator\\networks\\trainer.py:539\u001B[0m, in \u001B[0;36mWGANGPTrainer._train_epoch\u001B[1;34m(self, train_dataloader)\u001B[0m\n\u001B[0;32m    537\u001B[0m input_images \u001B[38;5;241m=\u001B[39m input_images\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    538\u001B[0m real_images \u001B[38;5;241m=\u001B[39m real_images\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m--> 539\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_discriminator\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreal_images\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    541\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m iteration \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    542\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_generator(input_images, real_images)\n",
      "File \u001B[1;32mC:\\python\\SMBackEnd\\LookGenerator\\networks\\trainer.py:584\u001B[0m, in \u001B[0;36mWGANGPTrainer._train_discriminator\u001B[1;34m(self, input_images, real_images)\u001B[0m\n\u001B[0;32m    581\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscriminator_fake_epoch_batches_loss\u001B[38;5;241m.\u001B[39mappend(torch\u001B[38;5;241m.\u001B[39mmean(fake_loss)\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m    583\u001B[0m \u001B[38;5;66;03m# Loss computation\u001B[39;00m\n\u001B[1;32m--> 584\u001B[0m gp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient_penalty\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfake_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreal_images\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    585\u001B[0m loss_discriminator \u001B[38;5;241m=\u001B[39m real_loss \u001B[38;5;241m+\u001B[39m fake_loss \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgp_weight \u001B[38;5;241m*\u001B[39m gp\n\u001B[0;32m    586\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscriminator_history_batches\u001B[38;5;241m.\u001B[39mappend(torch\u001B[38;5;241m.\u001B[39mmean(loss_discriminator)\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[1;31mTypeError\u001B[0m: GradientPenalty.forward() missing 2 required positional arguments: 'real_image' and 'device'"
     ]
    }
   ],
   "source": [
    "history = trainer.train(\n",
    "    train_dataloader=train_dataloader,\n",
    "    epoch_num=epoch_num\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:58:04.125625600Z",
     "start_time": "2023-06-02T11:57:59.715042500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.draw_history_plots()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plots_save_dir = r\"\"\n",
    "trainer.save_history_plots(plots_save_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "readme_save_dir = r\"\"\n",
    "trainer.create_readme(readme_save_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "test_dataset = EncoderDecoderDataset(\n",
    "    image_dir=r\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoder\\val\",\n",
    "    transform_human=transform_input,\n",
    "    transform_clothes=transform_input,\n",
    "    transform_human_restored=transform_real\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T01:02:58.188477Z",
     "end_time": "2023-04-11T01:02:58.193506Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator.to('cpu')\n",
    "discriminator.to('cpu')\n",
    "image, real_image = test_dataset[2]\n",
    "image = image.unsqueeze(0)\n",
    "#print(image)\n",
    "print(image.shape)\n",
    "image = generator(image)\n",
    "imaged = discriminator(image)\n",
    "image = transforms.ToPILImage()(image[0, :, :, :])\n",
    "image.show()\n",
    "print(imaged)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T01:02:58.197201Z",
     "end_time": "2023-04-11T01:03:01.725081Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
