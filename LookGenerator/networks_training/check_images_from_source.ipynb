{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-03T00:02:09.082972600Z",
     "start_time": "2023-06-03T00:02:09.081467300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from LookGenerator.networks.segmentation import UNet\n",
    "from LookGenerator.networks.bpgm.model.models import BPGM\n",
    "from LookGenerator.networks.clothes_feature_extractor import ClothAutoencoder\n",
    "from LookGenerator.networks.encoder_decoder import EncoderDecoder\n",
    "\n",
    "import LookGenerator.datasets.transforms as custom_transforms\n",
    "from LookGenerator.datasets.utils import prepare_image_for_segmentation\n",
    "from LookGenerator.networks.utils import load_model\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "human_path = r'C:\\Users\\DenisovDmitrii\\Desktop\\zalando-hd-resize\\test\\image'\n",
    "cloth_path = r'C:\\Users\\DenisovDmitrii\\Desktop\\zalando-hd-resize\\train\\cloth'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "segmentation_bin_path = r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\segmetationBackground\\weights\\testResults\\epoch_29.pt\"\n",
    "\n",
    "segmentation_multy_path = r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\segmentationMulty\\weights\\testMulty_out_12_6features\\epoch_29.pt\"\n",
    "\n",
    "tps_path = r'C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\tps\\weights\\test\\epoch_02.pt'\n",
    "\n",
    "clothes_feature_extractor_path = r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\autoDegradation\\weights\\testClothes_L1Loss_4features\\epoch_39.pt\"\n",
    "\n",
    "encoder_path = r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\newEncoder\\weights\\testWithTPSMask\\epoch_29.pt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:59:55.668864400Z",
     "start_time": "2023-06-02T23:59:55.661766Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "11647"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloth_list = os.listdir(cloth_path)\n",
    "len(cloth_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:59:55.681500900Z",
     "start_time": "2023-06-02T23:59:55.668864400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "2032"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_list = os.listdir(human_path)\n",
    "len(human_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:59:55.685222700Z",
     "start_time": "2023-06-02T23:59:55.680497900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:59:55.736770Z",
     "start_time": "2023-06-02T23:59:55.684219100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "transforms_resize = transforms.Compose([\n",
    "    transforms.Resize((256, 192))\n",
    "])\n",
    "\n",
    "transform_input_segmentation = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.25, 0.25, 0.25]\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_for_tps_and_encoder = transforms.Compose([\n",
    "    transforms.Resize((256,192)),\n",
    "   transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_output_segmentation = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    custom_transforms.MinMaxScale(),\n",
    "    custom_transforms.ThresholdTransform(threshold=0.5)\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:59:55.736770Z",
     "start_time": "2023-06-02T23:59:55.697864600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "toTensor = transforms.ToTensor()\n",
    "toPIL = transforms.ToPILImage()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:59:55.736770Z",
     "start_time": "2023-06-02T23:59:55.707397Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "segmentation_bin = UNet(in_channels=3, out_channels=1)\n",
    "segmentation_bin = load_model(segmentation_bin, segmentation_bin_path)\n",
    "segmentation_bin = segmentation_bin.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:59:55.892208700Z",
     "start_time": "2023-06-02T23:59:55.722721Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "segmentation_multy = UNet(in_channels=3, out_channels=12,\n",
    "                          features=(16, 32, 64, 128, 256, 512),\n",
    "                          final_activation=nn.Softmax(dim=1))\n",
    "segmentation_multy = load_model(segmentation_multy, segmentation_multy_path)\n",
    "segmentation_multy = segmentation_multy.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T00:00:20.383158200Z",
     "start_time": "2023-06-03T00:00:20.273846200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization method [normal]\n",
      "initialization method [normal]\n"
     ]
    }
   ],
   "source": [
    "tps = BPGM(in_channels=12, device=device)\n",
    "tps = load_model(tps, tps_path)\n",
    "tps = tps.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T00:00:20.946141600Z",
     "start_time": "2023-06-03T00:00:20.807177800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "clothes_feature_extractor = ClothAutoencoder(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    features=(8, 16, 32, 64),\n",
    "    latent_dim_size=128,\n",
    "    encoder_activation_func=nn.LeakyReLU(),\n",
    "    decoder_activation_func=nn.ReLU()\n",
    ")\n",
    "clothes_feature_extractor = load_model(clothes_feature_extractor, clothes_feature_extractor_path)\n",
    "clothes_feature_extractor = clothes_feature_extractor.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T00:00:21.385534200Z",
     "start_time": "2023-06-03T00:00:21.369530100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "encoder_decoder = EncoderDecoder(clothes_feature_extractor, in_channels=6, out_channels=3)\n",
    "encoder_decoder = load_model(encoder_decoder, encoder_path)\n",
    "encoder_decoder = encoder_decoder.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T00:00:22.161697200Z",
     "start_time": "2023-06-03T00:00:22.086198300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2032 [00:00<?, ?it/s]C:\\Users\\DenisovDmitrii\\AppData\\Local\\Temp\\ipykernel_14644\\1250078024.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segmentation_bin_out_bool = torch.tensor(segmentation_bin_out, dtype=torch.bool)\n",
      "C:\\Users\\DenisovDmitrii\\AppData\\Local\\Temp\\ipykernel_14644\\1250078024.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cwm = torch.tensor(cwm, dtype=torch.bool).to('cpu')\n",
      "100%|██████████| 2032/2032 [01:48<00:00, 18.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for human in tqdm(human_list):\n",
    "    number_cloth = np.random.randint(0, 11647)\n",
    "    # print(number_cloth)\n",
    "\n",
    "    cloth = cloth_list[number_cloth]\n",
    "\n",
    "    human_image = Image.open(os.path.join(human_path, human))\n",
    "    human_image = transforms_resize(toTensor(human_image).unsqueeze(0))\n",
    "    img_to_segmentation = transform_input_segmentation(human_image).to(device)\n",
    "\n",
    "    cloth_image = Image.open(os.path.join(cloth_path, cloth))\n",
    "    cloth_image = toTensor(cloth_image).unsqueeze(0)\n",
    "    cloth_to_model = transform_for_tps_and_encoder(cloth_image).to(device)\n",
    "\n",
    "    segmentation_bin_out = transform_output_segmentation(segmentation_bin(img_to_segmentation).detach().to('cpu'))\n",
    "    segmentation_bin_out_bool = torch.tensor(segmentation_bin_out, dtype=torch.bool)\n",
    "    segmentation_bin_out_clear = human_image * (~segmentation_bin_out_bool) + segmentation_bin_out_bool\n",
    "\n",
    "    segmentation_multy_out = transform_output_segmentation(segmentation_multy(img_to_segmentation).detach().to('cpu')).to(device)\n",
    "\n",
    "    cwm = segmentation_multy_out[:,8,:,:]\n",
    "    cwm = torch.tensor(cwm, dtype=torch.bool).to('cpu')\n",
    "    theta = tps(segmentation_multy_out, cloth_to_model)\n",
    "\n",
    "    warped = functional.grid_sample(cloth_to_model, theta, padding_mode='border', align_corners=True).to('cpu')\n",
    "    warped = warped / 2 + 0.5\n",
    "    warped = warped * cwm\n",
    "    person = segmentation_bin_out_clear * (~cwm) + warped\n",
    "\n",
    "    human_for_encoder = transform_for_tps_and_encoder(person).to(device)\n",
    "    data_to_encoder = torch.cat((human_for_encoder, cloth_to_model), dim=1)\n",
    "    model_out_from_encoder = encoder_decoder(data_to_encoder).to('cpu')\n",
    "    torchvision.utils.save_image(model_out_from_encoder, fr\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoderNew\\sameOut\\{human[:-4]}_{cloth[:-4]}.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T00:06:42.985190500Z",
     "start_time": "2023-06-03T00:04:54.628966900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:45:22.383690700Z",
     "start_time": "2023-06-02T23:45:22.215972800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "segmentation_bin_out_clear_cpu = segmentation_bin_out_clear.to('cpu')\n",
    "for img in segmentation_bin_out_clear_cpu:\n",
    "    toPIL(img).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:42:28.480965500Z",
     "start_time": "2023-06-02T23:42:25.196131400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "segmentation_bin_out_cpu = segmentation_bin_out.to('cpu')\n",
    "for img in segmentation_bin_out_cpu:\n",
    "    toPIL(img).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T21:48:13.187320400Z",
     "start_time": "2023-06-02T21:48:10.076766500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[100], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m segmentation_multy_out_cpu:\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chanel \u001B[38;5;129;01min\u001B[39;00m img:\n\u001B[1;32m----> 4\u001B[0m         \u001B[43mtoPIL\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchanel\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:2485\u001B[0m, in \u001B[0;36mImage.show\u001B[1;34m(self, title)\u001B[0m\n\u001B[0;32m   2465\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, title\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   2466\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2467\u001B[0m \u001B[38;5;124;03m    Displays this image. This method is mainly intended for debugging purposes.\u001B[39;00m\n\u001B[0;32m   2468\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2482\u001B[0m \u001B[38;5;124;03m    :param title: Optional title to use for the image window, where possible.\u001B[39;00m\n\u001B[0;32m   2483\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2485\u001B[0m     \u001B[43m_show\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtitle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtitle\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3508\u001B[0m, in \u001B[0;36m_show\u001B[1;34m(image, **options)\u001B[0m\n\u001B[0;32m   3505\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_show\u001B[39m(image, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions):\n\u001B[0;32m   3506\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ImageShow\n\u001B[1;32m-> 3508\u001B[0m     \u001B[43mImageShow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageShow.py:62\u001B[0m, in \u001B[0;36mshow\u001B[1;34m(image, title, **options)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;124;03mDisplay a given image.\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;124;03m:returns: ``True`` if a suitable viewer was found, ``False`` otherwise.\u001B[39;00m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m viewer \u001B[38;5;129;01min\u001B[39;00m _viewers:\n\u001B[1;32m---> 62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mviewer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtitle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtitle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m     63\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageShow.py:86\u001B[0m, in \u001B[0;36mViewer.show\u001B[1;34m(self, image, **options)\u001B[0m\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m image\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m!=\u001B[39m base:\n\u001B[0;32m     84\u001B[0m         image \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mconvert(base)\n\u001B[1;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageShow.py:112\u001B[0m, in \u001B[0;36mViewer.show_image\u001B[1;34m(self, image, **options)\u001B[0m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow_image\u001B[39m(\u001B[38;5;28mself\u001B[39m, image, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions):\n\u001B[0;32m    111\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Display the given image.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 112\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow_file\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageShow.py:129\u001B[0m, in \u001B[0;36mViewer.show_file\u001B[1;34m(self, path, **options)\u001B[0m\n\u001B[0;32m    127\u001B[0m         msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    128\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 129\u001B[0m os\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_command(path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions))  \u001B[38;5;66;03m# nosec\u001B[39;00m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "segmentation_multy_out_cpu = segmentation_multy_out.to('cpu')\n",
    "for img in segmentation_multy_out_cpu:\n",
    "    for chanel in img:\n",
    "        toPIL(chanel).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:38:28.871254400Z",
     "start_time": "2023-06-02T23:38:25.575008300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "for img in person:\n",
    "    toPIL(img).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:39:08.059993Z",
     "start_time": "2023-06-02T23:39:04.789654700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "for img in cloth_image:\n",
    "    toPIL(img).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T00:00:48.571927100Z",
     "start_time": "2023-06-03T00:00:45.369743Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "for img in model_out_from_encoder:\n",
    "    toPIL(img).show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T00:00:45.369743Z",
     "start_time": "2023-06-03T00:00:42.252202Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "for img in data_to_encoder[:, :3, :, :]:\n",
    "    toPIL(img).show()\n",
    "for img in data_to_encoder[:, 3:, :, :]:\n",
    "    toPIL(img).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:43:02.378233500Z",
     "start_time": "2023-06-02T23:42:56.121124100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:18:32.996012300Z",
     "start_time": "2023-06-02T23:18:29.676735800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
