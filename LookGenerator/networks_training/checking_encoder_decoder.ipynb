{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL.Image as Image\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import LookGenerator.datasets.transforms as custom_transforms\n",
    "from LookGenerator.datasets.utils import prepare_images_for_encoder, load_image\n",
    "from LookGenerator.networks.encoder_decoder import EncoderDecoder\n",
    "from LookGenerator.networks.clothes_feature_extractor import ClothAutoencoder\n",
    "from LookGenerator.networks.utils import load_model\n",
    "import cv2\n",
    "from tqdm import tqdm\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:35:21.763444400Z",
     "start_time": "2023-05-31T14:35:21.761940300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Загрузка изображений"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:05.602990200Z",
     "start_time": "2023-05-31T14:36:05.600328400Z"
    }
   },
   "outputs": [],
   "source": [
    "#root = r\"C:\\Users\\Даша\\кто\\мусор\\zalando-hd-resized\\val\"\n",
    "root = r\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoderNew\\val\"\n",
    "file_name = r\"00006_00\"\n",
    "\n",
    "human_image = load_image(root, r\"imageWithNoCloth\", file_name, \".png\")\n",
    "clothes = load_image(root, r\"cloth\", file_name, r\".jpg\")\n",
    "\n",
    "# pose_points = []\n",
    "# points_list = os.listdir(os.path.join(\n",
    "#     root,\n",
    "#     r\"posePoints\",\n",
    "#     file_name\n",
    "# ))\n",
    "# print(len(points_list))\n",
    "# for pose_point in points_list:\n",
    "#     pose_point_image = convert_channel(load_image(root, os.path.join(r\"posePoints\", file_name), pose_point, \"\"))\n",
    "#     pose_points.append(pose_point_image)\n",
    "\n",
    "# if model dataset has pose_points=False param:\n",
    "# pose_points = []"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Определение трансформаций"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "transform_human = transforms.Compose([\n",
    "    transforms.Resize((256, 192)) #,\n",
    "    # transforms.RandomAffine(scale=(0.8, 1), degrees=(-90,90), fill = 0.9),\n",
    "    # transforms.ColorJitter(brightness=(0.5, 1), contrast=(0.4,1),  hue=(0, 0.3)),\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "    #                      std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "transform_clothes = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    # transforms.ColorJitter(brightness=(0.5, 1), contrast=(0.4,1),  hue=(0, 0.3)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# input_bin_transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 192)),\n",
    "#     custom_transforms.Normalize()\n",
    "# ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:05.943261400Z",
     "start_time": "2023-05-31T14:36:05.938747Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "totensor = transforms.ToTensor()\n",
    "human_image = transform_human(totensor(human_image))\n",
    "clothes_image = transform_clothes(totensor(clothes))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:06.131091300Z",
     "start_time": "2023-05-31T14:36:06.118046100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Загрузка весов модели"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "model_input = torch.cat((human_image, clothes_image), axis=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:06.841562200Z",
     "start_time": "2023-05-31T14:36:06.834108400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([6, 256, 192])"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:07.075870100Z",
     "start_time": "2023-05-31T14:36:07.069433600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "model_input = model_input.unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:07.291408900Z",
     "start_time": "2023-05-31T14:36:07.284887400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 6, 256, 192])"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:07.491381Z",
     "start_time": "2023-05-31T14:36:07.484466100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# model_input = prepare_images_for_encoder(human_image, pose_points, clothes, input_rgb_transform, input_bin_transform).float()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:07.783666300Z",
     "start_time": "2023-05-31T14:36:07.782161200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "#weights_dir = r\"C:\\Users\\Даша\\PycharmProjects\\SMBackEnd\\LookGenerator\\weights\\epoch_19.pt\"\n",
    "weights_dir = r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\newEncoder\\weights\\testWithMask_weights102\\epoch_09.pt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:08.164467100Z",
     "start_time": "2023-05-31T14:36:08.157945Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "clothes_feature_extractor = ClothAutoencoder(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    features=(8, 16, 32, 64),\n",
    "    latent_dim_size=128,\n",
    "    encoder_activation_func=nn.LeakyReLU(),\n",
    "    decoder_activation_func=nn.ReLU()\n",
    ")\n",
    "clothes_feature_extractor = load_model(clothes_feature_extractor, r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\autoDegradation\\weights\\testClothes_L1Loss_4features\\epoch_39.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:08.359517800Z",
     "start_time": "2023-05-31T14:36:08.348428800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "model = EncoderDecoder(clothes_feature_extractor, in_channels=6, out_channels=3)\n",
    "model = load_model(model, weights_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:08.651001300Z",
     "start_time": "2023-05-31T14:36:08.581281500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "model_input = model_input.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:08.784401500Z",
     "start_time": "2023-05-31T14:36:08.780449900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Прогон модели"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "save_path = fr\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoderNew\\{file_name}.png\"\n",
    "save_path_old = fr\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoder\\out\\{file_name}_old.png\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:09.146163700Z",
     "start_time": "2023-05-31T14:36:09.138606600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "model_output = model(model_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:09.558403400Z",
     "start_time": "2023-05-31T14:36:09.404534600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "torchvision.utils.save_image(model_output, save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T14:36:09.832221100Z",
     "start_time": "2023-05-31T14:36:09.814232900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Отображение результата"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "model_output = to_array_from_decoder(model_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:38:27.467146Z",
     "end_time": "2023-04-03T21:38:27.486930Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "to_plt = np.array(model_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:38:27.662538Z",
     "end_time": "2023-04-03T21:38:27.672573Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "c = np.array(to_plt[:,:,0])\n",
    "to_plt[:,:,0] = to_plt[:,:,2]\n",
    "to_plt[:,:,2] = c"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:38:27.879312Z",
     "end_time": "2023-04-03T21:38:27.887840Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_array_as_image(np.invert(np.uint8(255*to_plt)))\n",
    "#save_array_as_image(np.uint8(255*model_output), save_path=save_path_old)\n",
    "# cv2.imwrite(fr\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoder\\out\\{file_name}.png\", cv2.cvtColor(np.uint8(255*model_output), cv2.COLOR_RGB2BGR) )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-02T23:52:06.025677Z",
     "end_time": "2023-04-02T23:52:06.106783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "dir_ = r\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoder\\val\\imageWithNoCloth\"\n",
    "list_files = os.listdir(dir_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:49:40.683852Z",
     "end_time": "2023-04-03T21:49:40.708617Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "input_rgb_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    custom_transforms.Normalize()\n",
    "])\n",
    "\n",
    "input_bin_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    custom_transforms.Normalize()\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:49:41.465580Z",
     "end_time": "2023-04-03T21:49:41.491111Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "weights_dir = r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\encoder\\weights\\session19_2\\epoch_01.pt\"\n",
    "model = EncoderDecoder(in_channels=6, out_channels=3)\n",
    "model = load_model(model, weights_dir)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:49:42.961137Z",
     "end_time": "2023-04-03T21:49:43.118150Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for file_name_ in tqdm(list_files[:150]):\n",
    "    root = r\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoder\\val\"\n",
    "    file_name = file_name_[:-4]\n",
    "\n",
    "    human_image = load_image(root, r\"imageWithNoCloth\", file_name, \".png\")\n",
    "    clothes = load_image(root, r\"cloth\", file_name, r\".jpg\")\n",
    "\n",
    "    pose_points = []\n",
    "    points_list = os.listdir(os.path.join(\n",
    "        root,\n",
    "        r\"posePoints\",\n",
    "        file_name\n",
    "    ))\n",
    "    for pose_point in points_list:\n",
    "        pose_point_image = convert_channel(load_image(root, os.path.join(r\"posePoints\", file_name), pose_point, \"\"))\n",
    "        pose_points.append(pose_point_image)\n",
    "\n",
    "    # if model dataset has pose_points=False param:\n",
    "    pose_points = []\n",
    "    model_input = prepare_images_for_encoder(human_image, pose_points, clothes, input_rgb_transform, input_bin_transform).float()\n",
    "    model_input = model_input.to(device)\n",
    "\n",
    "    save_path = fr\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoder\\out\\{file_name}.png\"\n",
    "    model_output = model(model_input)\n",
    "    torchvision.utils.save_image(model_output.to('cpu'), save_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:49:47.689895Z",
     "end_time": "2023-04-03T21:50:00.288271Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
