{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:06:40.246728500Z",
     "start_time": "2023-06-05T21:06:39.229875200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from LookGenerator.networks.segmentation import UNet\n",
    "from LookGenerator.networks.bpgm.model.models import BPGM\n",
    "from LookGenerator.networks.clothes_feature_extractor import ClothAutoencoder\n",
    "from LookGenerator.networks.encoder_decoder import EncoderDecoder\n",
    "\n",
    "import LookGenerator.datasets.transforms as custom_transforms\n",
    "from LookGenerator.datasets.utils import prepare_image_for_segmentation\n",
    "from LookGenerator.networks.utils import load_model\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "human_path = r'C:\\Users\\DenisovDmitrii\\Desktop\\12channels\\valData\\som'\n",
    "cloth_path = r'C:\\Users\\DenisovDmitrii\\Desktop\\zalando-hd-resize\\train\\cloth'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:06:40.250292600Z",
     "start_time": "2023-06-05T21:06:40.247278100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "segmentation_bin_path = r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\segmetationBackground\\weights\\testResultsFeatures_32_64_128_256_512\\epoch_39.pt\"\n",
    "\n",
    "segmentation_multy_path = r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\segmentationMulty\\weights\\testMulty_out_12_6features_20to640_fillBack\\epoch_37.pt\"\n",
    "\n",
    "tps_path = r'C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\tps\\weights\\test\\epoch_02.pt'\n",
    "\n",
    "clothes_feature_extractor_path = r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\autoDegradation\\weights\\testClothes_L1Loss_4features\\epoch_39.pt\"\n",
    "\n",
    "encoder_path = r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\newEncoder\\weights\\testWithTPSMask\\epoch_29.pt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:12:26.172067200Z",
     "start_time": "2023-06-05T21:12:26.161531400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "11647"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloth_list = os.listdir(cloth_path)\n",
    "len(cloth_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:12:26.491106100Z",
     "start_time": "2023-06-05T21:12:26.478558400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "18"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_list = os.listdir(human_path)\n",
    "len(human_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:12:26.760144900Z",
     "start_time": "2023-06-05T21:12:26.757976200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:12:27.472280700Z",
     "start_time": "2023-06-05T21:12:27.460847800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "transforms_resize = transforms.Compose([\n",
    "    transforms.Resize((256, 192))\n",
    "])\n",
    "\n",
    "transform_input_segmentation = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.25, 0.25, 0.25]\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_for_tps_and_encoder = transforms.Compose([\n",
    "    transforms.Resize((256,192)),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_output_segmentation = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    custom_transforms.MinMaxScale(),\n",
    "    custom_transforms.ThresholdTransform(threshold=0.5)\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:12:27.647424Z",
     "start_time": "2023-06-05T21:12:27.637432900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "toTensor = transforms.ToTensor()\n",
    "toPIL = transforms.ToPILImage()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:12:28.027482100Z",
     "start_time": "2023-06-05T21:12:28.016946700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "segmentation_bin = UNet(in_channels=3, out_channels=1, features=(32, 64, 128, 256, 512))\n",
    "segmentation_bin = load_model(segmentation_bin, segmentation_bin_path)\n",
    "segmentation_bin = segmentation_bin.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:12:28.317739800Z",
     "start_time": "2023-06-05T21:12:28.219985500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "segmentation_multy = UNet(in_channels=3, out_channels=12,\n",
    "                          features=(20, 40, 80, 160, 320, 640),\n",
    "                          final_activation=nn.Softmax(dim=1))\n",
    "segmentation_multy = load_model(segmentation_multy, segmentation_multy_path)\n",
    "segmentation_multy = segmentation_multy.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:12:28.613781200Z",
     "start_time": "2023-06-05T21:12:28.419771900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization method [normal]\n",
      "initialization method [normal]\n"
     ]
    }
   ],
   "source": [
    "tps = BPGM(in_channels=12, device=device)\n",
    "tps = load_model(tps, tps_path)\n",
    "tps = tps.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:12:28.761292Z",
     "start_time": "2023-06-05T21:12:28.612777700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "clothes_feature_extractor = ClothAutoencoder(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    features=(8, 16, 32, 64),\n",
    "    latent_dim_size=128,\n",
    "    encoder_activation_func=nn.LeakyReLU(),\n",
    "    decoder_activation_func=nn.ReLU()\n",
    ")\n",
    "clothes_feature_extractor = load_model(clothes_feature_extractor, clothes_feature_extractor_path)\n",
    "clothes_feature_extractor = clothes_feature_extractor.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:12:28.853763Z",
     "start_time": "2023-06-05T21:12:28.829679500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "encoder_decoder = EncoderDecoder(clothes_feature_extractor, in_channels=6, out_channels=3)\n",
    "encoder_decoder = load_model(encoder_decoder, encoder_path)\n",
    "encoder_decoder = encoder_decoder.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:12:29.321375600Z",
     "start_time": "2023-06-05T21:12:29.242715300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]C:\\Users\\DenisovDmitrii\\AppData\\Local\\Temp\\ipykernel_28188\\1366298862.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  segmentation_bin_out_bool = torch.tensor(segmentation_bin_out, dtype=torch.bool)\n",
      "C:\\Users\\DenisovDmitrii\\AppData\\Local\\Temp\\ipykernel_28188\\1366298862.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cwm = torch.tensor(cwm, dtype=torch.bool)\n",
      "100%|██████████| 18/18 [00:01<00:00, 17.87it/s]\n"
     ]
    }
   ],
   "source": [
    "for human in tqdm(human_list):\n",
    "    number_cloth = np.random.randint(0, 11647)\n",
    "    # print(number_cloth)\n",
    "\n",
    "    cloth = cloth_list[number_cloth]\n",
    "\n",
    "    human_image = Image.open(os.path.join(human_path, human))\n",
    "    human_image = transforms_resize(toTensor(human_image).unsqueeze(0)).to(device)\n",
    "    img_to_segmentation = transform_input_segmentation(human_image)\n",
    "\n",
    "    cloth_image = Image.open(os.path.join(cloth_path, cloth))\n",
    "    cloth_image = toTensor(cloth_image).unsqueeze(0).to(device)\n",
    "    cloth_to_model = transform_for_tps_and_encoder(cloth_image)\n",
    "\n",
    "    segmentation_bin_out = transform_output_segmentation(segmentation_bin(img_to_segmentation).detach())\n",
    "    segmentation_bin_out_bool = torch.tensor(segmentation_bin_out, dtype=torch.bool)\n",
    "    segmentation_bin_out_clear = human_image * (~segmentation_bin_out_bool) + segmentation_bin_out_bool\n",
    "\n",
    "    segmentation_multy_out = transform_output_segmentation(segmentation_multy(img_to_segmentation).detach())\n",
    "\n",
    "    cwm = segmentation_multy_out[:,8,:,:]\n",
    "    cwm = torch.tensor(cwm, dtype=torch.bool)\n",
    "    theta = tps(segmentation_multy_out, cloth_to_model)\n",
    "\n",
    "    warped = functional.grid_sample(cloth_to_model, theta, padding_mode='border', align_corners=True)\n",
    "    warped = warped / 2 + 0.5\n",
    "    warped = warped * cwm\n",
    "    person = segmentation_bin_out_clear * (~cwm) + warped\n",
    "\n",
    "    human_for_encoder = transform_for_tps_and_encoder(person)\n",
    "    data_to_encoder = torch.cat((human_for_encoder, cloth_to_model), dim=1)\n",
    "    model_out_from_encoder = encoder_decoder(data_to_encoder).to('cpu')\n",
    "    # segmentation_bin_out_clear_cpu = segmentation_bin_out_clear.to('cpu')\n",
    "    torchvision.utils.save_image(model_out_from_encoder, fr\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoderNew\\sameOut2\\{human[:-4]}_{cloth[:-4]}.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T21:13:18.866030500Z",
     "start_time": "2023-06-05T21:13:17.850796600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:45:22.383690700Z",
     "start_time": "2023-06-02T23:45:22.215972800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "segmentation_bin_out_clear_cpu = segmentation_bin_out_clear.to('cpu')\n",
    "for img in segmentation_bin_out_clear_cpu:\n",
    "    toPIL(img).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:42:28.480965500Z",
     "start_time": "2023-06-02T23:42:25.196131400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "segmentation_bin_out_cpu = segmentation_bin_out.to('cpu')\n",
    "for img in segmentation_bin_out_cpu:\n",
    "    toPIL(img).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T21:48:13.187320400Z",
     "start_time": "2023-06-02T21:48:10.076766500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segmentation_multy_out_cpu = segmentation_multy_out.to('cpu')\n",
    "for img in segmentation_multy_out_cpu:\n",
    "    for chanel in img:\n",
    "        toPIL(chanel).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:38:28.871254400Z",
     "start_time": "2023-06-02T23:38:25.575008300Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "for img in person:\n",
    "    toPIL(img).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:39:08.059993Z",
     "start_time": "2023-06-02T23:39:04.789654700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "for img in cloth_image:\n",
    "    toPIL(img).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T00:00:48.571927100Z",
     "start_time": "2023-06-03T00:00:45.369743Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "for img in model_out_from_encoder:\n",
    "    toPIL(img).show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T00:00:45.369743Z",
     "start_time": "2023-06-03T00:00:42.252202Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "for img in data_to_encoder[:, :3, :, :]:\n",
    "    toPIL(img).show()\n",
    "for img in data_to_encoder[:, 3:, :, :]:\n",
    "    toPIL(img).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:43:02.378233500Z",
     "start_time": "2023-06-02T23:42:56.121124100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:18:32.996012300Z",
     "start_time": "2023-06-02T23:18:29.676735800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
