{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Импорты"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL.Image as Image\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import LookGenerator.datasets.transforms as custom_transforms\n",
    "from LookGenerator.datasets.utils import prepare_images_for_encoder, load_image\n",
    "from LookGenerator.networks.encoder_decoder import EncoderDecoder\n",
    "from LookGenerator.networks.clothes_feature_extractor import ClothAutoencoder\n",
    "from LookGenerator.networks.utils import load_model\n",
    "import cv2\n",
    "from tqdm import tqdm\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:29:43.313093400Z",
     "start_time": "2023-06-02T23:29:42.095589300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Загрузка изображений"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:39:53.524751900Z",
     "start_time": "2023-06-02T23:39:53.522362600Z"
    }
   },
   "outputs": [],
   "source": [
    "#root = r\"C:\\Users\\Даша\\кто\\мусор\\zalando-hd-resized\\val\"\n",
    "root = r\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoderNew\\val\"\n",
    "file_name = r\"00006_00\"\n",
    "\n",
    "human_image = load_image(root, r\"imageWithNoCloth2\", file_name, \".png\")\n",
    "clothes = load_image(root, r\"cloth\", file_name, r\".jpg\")\n",
    "\n",
    "# pose_points = []\n",
    "# points_list = os.listdir(os.path.join(\n",
    "#     root,\n",
    "#     r\"posePoints\",\n",
    "#     file_name\n",
    "# ))\n",
    "# print(len(points_list))\n",
    "# for pose_point in points_list:\n",
    "#     pose_point_image = convert_channel(load_image(root, os.path.join(r\"posePoints\", file_name), pose_point, \"\"))\n",
    "#     pose_points.append(pose_point_image)\n",
    "\n",
    "# if model dataset has pose_points=False param:\n",
    "# pose_points = []"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Определение трансформаций"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "transform_human = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    # transforms.RandomAffine(scale=(0.8, 1), degrees=(-90,90), fill = 0.9),\n",
    "    # transforms.ColorJitter(brightness=(0.5, 1), contrast=(0.4,1),  hue=(0, 0.3)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "transform_clothes = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    # transforms.ColorJitter(brightness=(0.5, 1), contrast=(0.4,1),  hue=(0, 0.3)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# input_bin_transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 192)),\n",
    "#     custom_transforms.Normalize()\n",
    "# ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:39:54.208289100Z",
     "start_time": "2023-06-02T23:39:54.198409200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DenisovDmitrii\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "totensor = transforms.ToTensor()\n",
    "human_image = transform_human(totensor(human_image))\n",
    "clothes_image = transform_clothes(totensor(clothes))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:39:54.571899600Z",
     "start_time": "2023-06-02T23:39:54.537529800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Загрузка весов модели"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "model_input = torch.cat((human_image, clothes_image), dim=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:39:58.590023900Z",
     "start_time": "2023-06-02T23:39:58.587502300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([6, 256, 192])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:39:59.476069400Z",
     "start_time": "2023-06-02T23:39:59.465031500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model_input = model_input.unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:39:59.747177800Z",
     "start_time": "2023-06-02T23:39:59.745672900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 6, 256, 192])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:40:00.162654300Z",
     "start_time": "2023-06-02T23:40:00.138689100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# model_input = prepare_images_for_encoder(human_image, pose_points, clothes, input_rgb_transform, input_bin_transform).float()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:40:00.562815200Z",
     "start_time": "2023-06-02T23:40:00.561310400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#weights_dir = r\"C:\\Users\\Даша\\PycharmProjects\\SMBackEnd\\LookGenerator\\weights\\epoch_19.pt\"\n",
    "weights_dir = r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\newEncoder\\weights\\testWithTPSMask\\epoch_29.pt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:40:00.941676400Z",
     "start_time": "2023-06-02T23:40:00.940171400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "clothes_feature_extractor = ClothAutoencoder(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    features=(8, 16, 32, 64),\n",
    "    latent_dim_size=128,\n",
    "    encoder_activation_func=nn.LeakyReLU(),\n",
    "    decoder_activation_func=nn.ReLU()\n",
    ")\n",
    "clothes_feature_extractor = load_model(clothes_feature_extractor, r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\autoDegradation\\weights\\testClothes_L1Loss_4features\\epoch_39.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:40:01.358713800Z",
     "start_time": "2023-06-02T23:40:01.330559200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "model = EncoderDecoder(clothes_feature_extractor, in_channels=6, out_channels=3)\n",
    "model = load_model(model, weights_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:40:01.766826500Z",
     "start_time": "2023-06-02T23:40:01.698113400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "model_input = model_input.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:40:02.284048Z",
     "start_time": "2023-06-02T23:40:02.267490300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Прогон модели"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "save_path = fr\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoderNew\\{file_name}.png\"\n",
    "save_path_old = fr\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoder\\out\\{file_name}_old.png\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:40:03.120129800Z",
     "start_time": "2023-06-02T23:40:03.118626Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DenisovDmitrii\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_output = model(model_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:40:03.548535500Z",
     "start_time": "2023-06-02T23:40:03.416322300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "torchvision.utils.save_image(model_output, save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T23:40:04.226328100Z",
     "start_time": "2023-06-02T23:40:04.210577500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Отображение результата"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "model_output = to_array_from_decoder(model_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:38:27.467146Z",
     "end_time": "2023-04-03T21:38:27.486930Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "to_plt = np.array(model_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:38:27.662538Z",
     "end_time": "2023-04-03T21:38:27.672573Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "c = np.array(to_plt[:,:,0])\n",
    "to_plt[:,:,0] = to_plt[:,:,2]\n",
    "to_plt[:,:,2] = c"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:38:27.879312Z",
     "end_time": "2023-04-03T21:38:27.887840Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_array_as_image(np.invert(np.uint8(255*to_plt)))\n",
    "#save_array_as_image(np.uint8(255*model_output), save_path=save_path_old)\n",
    "# cv2.imwrite(fr\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoder\\out\\{file_name}.png\", cv2.cvtColor(np.uint8(255*model_output), cv2.COLOR_RGB2BGR) )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-02T23:52:06.025677Z",
     "end_time": "2023-04-02T23:52:06.106783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "dir_ = r\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoder\\val\\imageWithNoCloth\"\n",
    "list_files = os.listdir(dir_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:49:40.683852Z",
     "end_time": "2023-04-03T21:49:40.708617Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "input_rgb_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    custom_transforms.Normalize()\n",
    "])\n",
    "\n",
    "input_bin_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    custom_transforms.Normalize()\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:49:41.465580Z",
     "end_time": "2023-04-03T21:49:41.491111Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "weights_dir = r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\encoder\\weights\\session19_2\\epoch_01.pt\"\n",
    "model = EncoderDecoder(in_channels=6, out_channels=3)\n",
    "model = load_model(model, weights_dir)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:49:42.961137Z",
     "end_time": "2023-04-03T21:49:43.118150Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for file_name_ in tqdm(list_files[:150]):\n",
    "    root = r\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoder\\val\"\n",
    "    file_name = file_name_[:-4]\n",
    "\n",
    "    human_image = load_image(root, r\"imageWithNoCloth\", file_name, \".png\")\n",
    "    clothes = load_image(root, r\"cloth\", file_name, r\".jpg\")\n",
    "\n",
    "    pose_points = []\n",
    "    points_list = os.listdir(os.path.join(\n",
    "        root,\n",
    "        r\"posePoints\",\n",
    "        file_name\n",
    "    ))\n",
    "    for pose_point in points_list:\n",
    "        pose_point_image = convert_channel(load_image(root, os.path.join(r\"posePoints\", file_name), pose_point, \"\"))\n",
    "        pose_points.append(pose_point_image)\n",
    "\n",
    "    # if model dataset has pose_points=False param:\n",
    "    pose_points = []\n",
    "    model_input = prepare_images_for_encoder(human_image, pose_points, clothes, input_rgb_transform, input_bin_transform).float()\n",
    "    model_input = model_input.to(device)\n",
    "\n",
    "    save_path = fr\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoder\\out\\{file_name}.png\"\n",
    "    model_output = model(model_input)/2+0.5\n",
    "    torchvision.utils.save_image(model_output.to('cpu'), save_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:49:47.689895Z",
     "end_time": "2023-04-03T21:50:00.288271Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
