{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<p style=\"align: center;\"><img src=\"https://raw.githubusercontent.com/dimkablin/SegmentationSM/master/src/logo2.jpg\", width=550, height=300></p>\n",
    "\n",
    "<h1 style=\"text-align: center;\"><b>Backend SM</b></h1>\n",
    "<h2 style=\"text-align: center;\"><b>Поиск архитекутуры для сегментации объекта для проекта SM </b></h2>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_DIR\n",
      "BACKGROUND_DATASET\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from LookGenerator.config.config import DatasetConfig\n",
    "\n",
    "config = DatasetConfig(os.environ)\n",
    "config.show_details()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-17T19:22:53.515929Z",
     "end_time": "2023-05-17T19:22:53.568425Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimka\\PycharmProjects\\data\\background C:\\Users\\dimka\\PycharmProjects\\data\\zalando-hd-resized\\train\n"
     ]
    }
   ],
   "source": [
    "print(config.BACKGROUND_DATASET, config.DATASET_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T22:29:45.526564Z",
     "end_time": "2023-05-05T22:29:45.713680Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Инициализируем датасет и dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, ToPILImage, PILToTensor\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from LookGenerator.networks.segmentation import UNet\n",
    "import albumentations\n",
    "import torchvision.transforms as transforms\n",
    "import LookGenerator.datasets.transforms as custom_transforms\n",
    "from LookGenerator.networks.losses import FocalLossBin\n",
    "from LookGenerator.networks.trainer import Trainer\n",
    "from LookGenerator.networks_training.utils import check_path_and_creat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:27:38.377070600Z",
     "start_time": "2023-05-18T19:27:36.510923Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Напишем датасет, который будет в себе хранить названия всех файлов и загружать их в методе \\_\\_getitem__()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class OblDataset(Dataset):\n",
    "    \"\"\" Dataset for Segmentation upper body \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_root: str,\n",
    "                 mask_root: str,\n",
    "                 mask_layer: int = 0,\n",
    "                 image_extension=\"jpg\",\n",
    "                 transform_input=None,\n",
    "                 transform_output=None,\n",
    "                 augment=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_root (str) : path to images\n",
    "            mask_root (str) : images directory name\n",
    "            image_extension (str) : extension of any file\n",
    "            mask_layer (int) : mask color !!!NEED REFUCKTOR!!!\n",
    "        \"\"\"\n",
    "        #TODO: mask_layer form 1-d array to 3-d array.\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_root = image_root\n",
    "        self.mask_root = mask_root\n",
    "        self.mask_layer = mask_layer\n",
    "        self.image_extension = image_extension\n",
    "\n",
    "        self.transform_input = transform_input\n",
    "        self.transform_output = transform_output\n",
    "        self.augment = augment\n",
    "\n",
    "        names_of_files = os.listdir(self.image_root)\n",
    "        self._files_list = [name.split('.')[0] for name in names_of_files]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Opening image by index that considered self.names_of_files variable\n",
    "\n",
    "        Args:\n",
    "            idx: The index of data sample\n",
    "\n",
    "        Returns:\n",
    "            Return torch.Tensor that represent image\n",
    "                   torch.Tensor that represent mask\n",
    "        \"\"\"\n",
    "\n",
    "        _image = Image.open(os.path.join(\n",
    "                self.image_root,\n",
    "                self._files_list[idx])\n",
    "                    + \".\" + self.image_extension\n",
    "            )\n",
    "        _image = PILToTensor()(_image).float()\n",
    "\n",
    "        hsv_min = np.array((0, 0, 127), np.uint8)\n",
    "        hsv_max = np.array((0, 0, 128), np.uint8)\n",
    "        _mask = cv2.imread(os.path.join(\n",
    "                self.mask_root,\n",
    "                self._files_list[idx])\n",
    "                    + \".\" + self.image_extension, cv2.IMREAD_COLOR)\n",
    "        _mask = cv2.medianBlur(_mask, 45)\n",
    "        _mask = cv2.cvtColor(_mask, cv2.COLOR_BGR2HSV)\n",
    "        _mask = cv2.inRange(_mask, hsv_min, hsv_max)\n",
    "\n",
    "        _mask = torch.from_numpy(_mask).float()\n",
    "        # _mask = Image.open(os.path.join(\n",
    "        #         self.mask_root,\n",
    "        #         self._files_list[idx])\n",
    "        #             + \".\" + self.image_extension\n",
    "        #     )\n",
    "\n",
    "\n",
    "\n",
    "        # _mask = PILToTensor()(_mask)\n",
    "        #\n",
    "        # _mask = torch.where(_mask[0] == self.mask_layer, 1, 0)\n",
    "\n",
    "        if self.augment:\n",
    "            transformed = self.augment(image=_image,\n",
    "                                       mask=_mask)\n",
    "            _image = transformed['image']\n",
    "            _mask = transformed['mask']\n",
    "\n",
    "        if self.transform_input:\n",
    "            _image = self.transform_input(_image)\n",
    "\n",
    "        if self.transform_output:\n",
    "            _mask = self.transform_output(_mask)\n",
    "\n",
    "        return _image, _mask\n",
    "\n",
    "    def __get_files_list__(self):\n",
    "        \"\"\"\n",
    "        Returning list of file\n",
    "\n",
    "        Returns:\n",
    "            list with names of files\n",
    "        \"\"\"\n",
    "\n",
    "        return self._files_list\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return count of files\n",
    "\n",
    "        Return:\n",
    "            length of self._files_list\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self._files_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:29:48.784390100Z",
     "start_time": "2023-05-18T19:29:48.778955500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Передадим ему путь к исходным фотографиям X и маске Y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "transform_input = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.25, 0.25, 0.25]\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_output = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    custom_transforms.MinMaxScale(),\n",
    "    custom_transforms.ThresholdTransform(threshold=0.5)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:29:49.572995300Z",
     "start_time": "2023-05-18T19:29:49.568111100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "transform_image = albumentations.Compose([\n",
    "        albumentations.Resize(height=256, width=192),\n",
    "        albumentations.RandomBrightnessContrast(brightness_limit=(0.1,0.3), contrast_limit=(0.2,0.7), p =0.2),\n",
    "        albumentations.Equalize(p = 0.2),\n",
    "        albumentations.GaussNoise(p = 0.2),\n",
    "        albumentations.Affine(translate_percent=0.1, scale=(0.8, 1), rotate=(-90,90), p=0.2),\n",
    "        albumentations.Normalize(mean = (0.5, 0.5, 0.5), std = (0.25, 0.25, 0.25)),\n",
    "    ])\n",
    "\n",
    "transform_mask = albumentations.Compose([\n",
    "        albumentations.Resize(height=256, width=192),\n",
    "        albumentations.Normalize(mean = (0.5, 0.5, 0.5), std = (0.25, 0.25, 0.25)),\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:29:49.971482800Z",
     "start_time": "2023-05-18T19:29:49.966967100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:29:50.440983200Z",
     "start_time": "2023-05-18T19:29:50.412385100Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = OblDataset(image_root=os.path.join(r'C:\\Users\\DenisovDmitrii\\Desktop\\zalando-hd-resize\\train', \"image\"),\n",
    "                          mask_root=os.path.join(r'C:\\Users\\DenisovDmitrii\\Desktop\\zalando-hd-resize\\train', \"agnostic-v3.2\"),\n",
    "                          mask_layer=128,\n",
    "                          image_extension=\"jpg\",\n",
    "                           transform_input=transform_input,\n",
    "                           transform_output=transform_output)\n",
    "\n",
    "val_dataset = OblDataset(image_root=os.path.join(r'C:\\Users\\DenisovDmitrii\\Desktop\\zalando-hd-resize\\test', \"image\"),\n",
    "                          mask_root=os.path.join(r'C:\\Users\\DenisovDmitrii\\Desktop\\zalando-hd-resize\\test', \"agnostic-v3.2\"),\n",
    "                          mask_layer=128,\n",
    "                          image_extension=\"jpg\",\n",
    "                           transform_input=transform_input,\n",
    "                           transform_output=transform_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверим его корректность работы и выведем парочку"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _print(image, mask):\n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "\n",
    "    fig.add_subplot( 1, 2, 1)\n",
    "    plt.imshow(ToPILImage()(image))\n",
    "\n",
    "    fig.add_subplot( 1, 2, 2)\n",
    "    plt.imshow(mask, cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:29:51.222947500Z",
     "start_time": "2023-05-18T19:29:51.216319400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [1024] and output size of [256, 192]. Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m number \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m11647\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m image, mask \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getitem__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnumber\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(image\u001B[38;5;241m.\u001B[39msize(), mask\u001B[38;5;241m.\u001B[39msize(), number)\n\u001B[0;32m      4\u001B[0m _print(image, mask)\n",
      "Cell \u001B[1;32mIn[26], line 86\u001B[0m, in \u001B[0;36mOblDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     83\u001B[0m     _image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform_input(_image)\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform_output:\n\u001B[1;32m---> 86\u001B[0m     _mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _image, _mask\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001B[0m, in \u001B[0;36mResize.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m    354\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    356\u001B[0m \u001B[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    359\u001B[0m \u001B[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[0;32m    360\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 361\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mantialias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:492\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[0;32m    489\u001B[0m     pil_interpolation \u001B[38;5;241m=\u001B[39m pil_modes_mapping[interpolation]\n\u001B[0;32m    490\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F_pil\u001B[38;5;241m.\u001B[39mresize(img, size\u001B[38;5;241m=\u001B[39moutput_size, interpolation\u001B[38;5;241m=\u001B[39mpil_interpolation)\n\u001B[1;32m--> 492\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mantialias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mantialias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:467\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation, antialias)\u001B[0m\n\u001B[0;32m    464\u001B[0m \u001B[38;5;66;03m# Define align_corners to avoid warnings\u001B[39;00m\n\u001B[0;32m    465\u001B[0m align_corners \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m interpolation \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbilinear\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbicubic\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 467\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43minterpolate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malign_corners\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malign_corners\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mantialias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mantialias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    469\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m interpolation \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbicubic\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m out_dtype \u001B[38;5;241m==\u001B[39m torch\u001B[38;5;241m.\u001B[39muint8:\n\u001B[0;32m    470\u001B[0m     img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mclamp(\u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mmax\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m255\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3869\u001B[0m, in \u001B[0;36minterpolate\u001B[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001B[0m\n\u001B[0;32m   3867\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(size, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m   3868\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(size) \u001B[38;5;241m!=\u001B[39m dim:\n\u001B[1;32m-> 3869\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   3870\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3871\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput with spatial dimensions of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m:])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and output size of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3872\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3873\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput size in (o1, o2, ...,oK) format.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3874\u001B[0m \n\u001B[0;32m   3875\u001B[0m         )\n\u001B[0;32m   3876\u001B[0m     output_size \u001B[38;5;241m=\u001B[39m size\n\u001B[0;32m   3877\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mValueError\u001B[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [1024] and output size of [256, 192]. Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "number = np.random.randint(0, 11647)\n",
    "image, mask = train_dataset.__getitem__(number)\n",
    "print(image.size(), mask.size(), number)\n",
    "_print(image, mask)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:29:51.855163Z",
     "start_time": "2023-05-18T19:29:51.746271400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Поделим датасет на тренировочную, валидационную и тестовую"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "batch_size_train = 32\n",
    "batch_size_val = 32\n",
    "pin_memory = True\n",
    "num_workers = 0\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True, pin_memory=pin_memory, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=False, pin_memory=pin_memory, num_workers=num_workers)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:24:38.155536400Z",
     "start_time": "2023-05-18T19:24:38.150018600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Реализируем основной цикл обучения"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Обучение"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Architectures\n",
    "### 1. our Unet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "model_unet = UNet(in_channels=3, out_channels = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:25:14.591766700Z",
     "start_time": "2023-05-18T19:25:14.530745Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "#model = load_model(model, r'')                 # активировать, если модель надо загрузить и дообучить\n",
    "optimizer = torch.optim.Adam(model_unet.parameters(), lr = 1e-3)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "criterion = FocalLossBin()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:25:16.331927600Z",
     "start_time": "2023-05-18T19:25:16.326404600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\segmetationBackground\\weights\\testResults\"\n",
    "check_path_and_creat(save_directory)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:25:16.668526300Z",
     "start_time": "2023-05-18T19:25:16.662815400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "epoch_num = 3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:25:17.714624500Z",
     "start_time": "2023-05-18T19:25:17.710110700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model_=model_unet, optimizer=optimizer, criterion = criterion,\n",
    "    device=device, save_directory=save_directory, save_step=1, verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:25:18.107985700Z",
     "start_time": "2023-05-18T19:25:18.102461800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time 18-05-2023 22:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/364 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "image must be numpy array type",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[74], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch_num\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch_num\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\python\\SMBackEnd\\LookGenerator\\networks\\trainer.py:57\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, train_dataloader, val_dataloader, epoch_num)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstart time\u001B[39m\u001B[38;5;124m\"\u001B[39m, start\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch_num):\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[1;32m---> 57\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_history_epochs\u001B[38;5;241m.\u001B[39mappend(train_loss)\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose:\n",
      "File \u001B[1;32mC:\\python\\SMBackEnd\\LookGenerator\\networks\\trainer.py:97\u001B[0m, in \u001B[0;36mTrainer._train_epoch\u001B[1;34m(self, train_dataloader)\u001B[0m\n\u001B[0;32m     95\u001B[0m train_running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m---> 97\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data, targets \u001B[38;5;129;01min\u001B[39;00m tqdm(train_dataloader):\n\u001B[0;32m     98\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m     99\u001B[0m     targets \u001B[38;5;241m=\u001B[39m targets\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\std.py:1178\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1175\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1178\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1179\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1180\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1181\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    676\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    677\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 678\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    679\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    680\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[58], line 77\u001B[0m, in \u001B[0;36mOblDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;66;03m# _mask = Image.open(os.path.join(\u001B[39;00m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m#         self.mask_root,\u001B[39;00m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m#         self._files_list[idx])\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m# _mask = torch.where(_mask[0] == self.mask_layer, 1, 0)\u001B[39;00m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maugment:\n\u001B[1;32m---> 77\u001B[0m     transformed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maugment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_image\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m     _image \u001B[38;5;241m=\u001B[39m transformed[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     80\u001B[0m     _mask \u001B[38;5;241m=\u001B[39m transformed[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmask\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\albumentations\\core\\composition.py:190\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, force_apply, *args, **data)\u001B[0m\n\u001B[0;32m    188\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have to pass data to augmentations as named arguments, for example: aug(image=image)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_check_args:\n\u001B[1;32m--> 190\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_args\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(force_apply, (\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m)), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mforce_apply must have bool or int type\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    192\u001B[0m need_to_run \u001B[38;5;241m=\u001B[39m force_apply \u001B[38;5;129;01mor\u001B[39;00m random\u001B[38;5;241m.\u001B[39mrandom() \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\albumentations\\core\\composition.py:267\u001B[0m, in \u001B[0;36mCompose._check_args\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m internal_data_name \u001B[38;5;129;01min\u001B[39;00m checked_single:\n\u001B[0;32m    266\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[1;32m--> 267\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m must be numpy array type\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(data_name))\n\u001B[0;32m    268\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m internal_data_name \u001B[38;5;129;01min\u001B[39;00m checked_multi:\n\u001B[0;32m    269\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data:\n",
      "\u001B[1;31mTypeError\u001B[0m: image must be numpy array type"
     ]
    }
   ],
   "source": [
    "trainer.train(train_dataloader=train_dataloader, val_dataloader=val_dataloader, epoch_num=epoch_num)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-18T19:25:18.657707900Z",
     "start_time": "2023-05-18T19:25:18.483950700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.draw_history_plots()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Визуальная проверка результатов"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_losses(model_, dataloader_, **loss_fns_):\n",
    "    model_.eval()\n",
    "\n",
    "    losses = { key : 0 for key in loss_fns_.keys() }\n",
    "\n",
    "    for x_batch, y_batch in dataloader_:\n",
    "        y_pred = model_(x_batch)\n",
    "\n",
    "        for name, loss_fn in loss_fns_.items():\n",
    "            loss = loss_fn(y_batch, y_pred)\n",
    "            losses[name] += loss.item() * len(x_batch)\n",
    "\n",
    "    for name, loss in losses.item():\n",
    "        losses[name] /= len(dataloader_)\n",
    "\n",
    "    return losses\n",
    "\n",
    "def plot_results(model_, dataloader_):\n",
    "    for x_batch, y_batch in dataloader_:\n",
    "        y_hat = model_(x_batch)\n",
    "        for k in range(6):\n",
    "            plt.subplot(2, 6, k+1)\n",
    "            plt.imshow(np.rollaxis(x_batch[k].cpu().numpy(), 0, 3), cmap='gray')\n",
    "            plt.title('Real')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(2, 6, k+7)\n",
    "            plt.imshow(y_hat[k, 0], cmap='gray')\n",
    "            plt.title('output')\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. SegNet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SegNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_conv0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, padding=2, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, padding=2, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.pool0 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)  # 256 -> 128\n",
    "        self.enc_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, padding=2, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, padding=2, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True) # 128 -> 64\n",
    "        self.enc_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, padding=2, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, padding=2, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, padding=2, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True) # 64 -> 32\n",
    "        self.enc_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, padding=1, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, padding=1, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, padding=1, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True) # 32 -> 16\n",
    "\n",
    "        # bottleneck\n",
    "\n",
    "        self.b_conv_enc = nn.Sequential(\n",
    "             nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "             nn.BatchNorm2d(512),\n",
    "             nn.ReLU(),\n",
    "             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "             nn.BatchNorm2d(512),\n",
    "             nn.ReLU(),\n",
    "             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "             nn.BatchNorm2d(512),\n",
    "             nn.ReLU()\n",
    "             )\n",
    "\n",
    "        self.b_pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "        self.b_unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.b_conv_dec = nn.Sequential(\n",
    "             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "             nn.BatchNorm2d(512),\n",
    "             nn.ReLU(),\n",
    "             nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "             nn.BatchNorm2d(512),\n",
    "             nn.ReLU(),\n",
    "             nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1),\n",
    "             nn.BatchNorm2d(256),\n",
    "             nn.ReLU()\n",
    "             )\n",
    "\n",
    "        # decoder (upsampling)\n",
    "        self.upsample0 = nn.MaxUnpool2d(kernel_size=2, stride=2) # 16 -> 32\n",
    "\n",
    "        self.dec_conv0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.upsample1 = nn.MaxUnpool2d(kernel_size=2, stride=2) # 32 -> 64\n",
    "        self.dec_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.upsample2 = nn.MaxUnpool2d(kernel_size=2, stride=2)  # 64 -> 128\n",
    "        self.dec_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.upsample3 = nn.MaxUnpool2d(kernel_size=2, stride=2)  # 128 -> 256\n",
    "        self.dec_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        size0 = x.size() # torch.Size([25, 3, 256, 256])\n",
    "\n",
    "        e0, indices0 = self.pool0(self.enc_conv0(x))\n",
    "        size1 = e0.size() # torch.Size([25, 8, 128, 128])\n",
    "\n",
    "        e1, indices1 = self.pool1(self.enc_conv1(e0))\n",
    "        size2 = e1.size() # torch.Size([25, 16, 64, 64])\n",
    "\n",
    "        e2, indices2 = self.pool2(self.enc_conv2(e1))\n",
    "        size3 = e2.size() # torch.Size([25, 32, 32, 32])\n",
    "\n",
    "        e3, indices3 = self.pool3(self.enc_conv3(e2))\n",
    "        size4 = e3.size() # torch.Size([25, 64, 16, 16])\n",
    "\n",
    "        # bottleneck\n",
    "        b1, bottle_id = self.b_pool( self.b_conv_enc(e3))\n",
    "        b1_size = b1.size()\n",
    "\n",
    "        b3 = self.b_conv_dec(self.b_unpool(b1, bottle_id, output_size=size4))\n",
    "\n",
    "        # decoder\n",
    "        d0 = self.dec_conv0(self.upsample0(b3, indices3, output_size=size3))\n",
    "        d1 = self.dec_conv1(self.upsample1(d0, indices2, output_size=size2))\n",
    "        d2 = self.dec_conv2(self.upsample2(d1, indices1, output_size=size1))\n",
    "        d3 = self.dec_conv3(self.upsample3(d2, indices0, output_size=size0))  # no activation\n",
    "        return d3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. a few Unet's arhc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
