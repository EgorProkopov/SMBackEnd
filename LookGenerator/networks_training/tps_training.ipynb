{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:17:06.313063200Z",
     "start_time": "2023-06-01T12:17:05.087173700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from LookGenerator.datasets.tps_dataset import ShirtsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from LookGenerator.networks.bpgm.model.models import BPGM\n",
    "from LookGenerator.networks.bpgm.train_bpgm import train_bpgm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "from LookGenerator.networks_training.utils import check_path_and_creat\n",
    "from LookGenerator.networks.utils import save_model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,192)),\n",
    "   transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((256,192)),\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:17:06.316033900Z",
     "start_time": "2023-06-01T12:17:06.313063200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 0\n",
    "pin_memory = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:17:06.323884800Z",
     "start_time": "2023-06-01T12:17:06.316033900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataset = ShirtsDataset(\n",
    "    root=r'C:\\Users\\Даша\\кто\\мусор\\zalando-hd-resized\\train',\n",
    "    #r\"C:\\Users\\DenisovDmitrii\\Desktop\\zalando-hd-resize\\train\",\n",
    "    transform=transform,\n",
    "    transform_mask=mask_transform\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=pin_memory, num_workers=num_workers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:17:06.333370300Z",
     "start_time": "2023-06-01T12:17:06.324888800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m save_directory \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDenisovDmitrii\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mOneDrive - ITMO UNIVERSITY\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mpeopleDetector\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtps\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mweights\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mcheck_path_and_creat\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_directory\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\LookGenerator\\networks_training\\utils.py:8\u001B[0m, in \u001B[0;36mcheck_path_and_creat\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(path\u001B[38;5;241m.\u001B[39mrsplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]):\n\u001B[1;32m----> 8\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m\n\u001B[0;32m      9\u001B[0m os\u001B[38;5;241m.\u001B[39mmkdir(path)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "save_directory = r'C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\tps\\weights\\test'\n",
    "check_path_and_creat(save_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:17:06.342000400Z",
     "start_time": "2023-06-01T12:17:06.333370300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "for data in dataloader:\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:17:11.713478500Z",
     "start_time": "2023-06-01T12:17:06.336982600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'cpu'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:17:11.718496200Z",
     "start_time": "2023-06-01T12:17:11.713478500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialization method [normal]\n",
      "initialization method [normal]\n"
     ]
    }
   ],
   "source": [
    "model = BPGM(device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:17:11.858848Z",
     "start_time": "2023-06-01T12:17:11.717492900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Даша\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Даша\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  0%|          | 6/11647 [00:53<29:04:29,  8.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_bpgm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\LookGenerator\\networks\\bpgm\\train_bpgm.py:42\u001B[0m, in \u001B[0;36mtrain_bpgm\u001B[1;34m(dataloader, model, device, epochs)\u001B[0m\n\u001B[0;32m     40\u001B[0m (transforms\u001B[38;5;241m.\u001B[39mToPILImage()(warped[\u001B[38;5;241m0\u001B[39m]))\u001B[38;5;241m.\u001B[39mshow()\n\u001B[0;32m     41\u001B[0m warped \u001B[38;5;241m=\u001B[39m warped \u001B[38;5;241m*\u001B[39m cwm\n\u001B[1;32m---> 42\u001B[0m \u001B[43m(\u001B[49m\u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mToPILImage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwarped\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m warped_mask \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mgrid_sample(shirt_mask, grid, padding_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mborder\u001B[39m\u001B[38;5;124m'\u001B[39m, align_corners\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     45\u001B[0m loss_cloth \u001B[38;5;241m=\u001B[39m criterionL1(warped, warped_shirt) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0.1\u001B[39m \u001B[38;5;241m*\u001B[39m criterionVGG(warped, warped_shirt)\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\PIL\\Image.py:2485\u001B[0m, in \u001B[0;36mImage.show\u001B[1;34m(self, title)\u001B[0m\n\u001B[0;32m   2465\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, title\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   2466\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2467\u001B[0m \u001B[38;5;124;03m    Displays this image. This method is mainly intended for debugging purposes.\u001B[39;00m\n\u001B[0;32m   2468\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2482\u001B[0m \u001B[38;5;124;03m    :param title: Optional title to use for the image window, where possible.\u001B[39;00m\n\u001B[0;32m   2483\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2485\u001B[0m     \u001B[43m_show\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtitle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtitle\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\PIL\\Image.py:3508\u001B[0m, in \u001B[0;36m_show\u001B[1;34m(image, **options)\u001B[0m\n\u001B[0;32m   3505\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_show\u001B[39m(image, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions):\n\u001B[0;32m   3506\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ImageShow\n\u001B[1;32m-> 3508\u001B[0m     ImageShow\u001B[38;5;241m.\u001B[39mshow(image, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\PIL\\ImageShow.py:62\u001B[0m, in \u001B[0;36mshow\u001B[1;34m(image, title, **options)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;124;03mDisplay a given image.\u001B[39;00m\n\u001B[0;32m     55\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;124;03m:returns: ``True`` if a suitable viewer was found, ``False`` otherwise.\u001B[39;00m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m viewer \u001B[38;5;129;01min\u001B[39;00m _viewers:\n\u001B[1;32m---> 62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m viewer\u001B[38;5;241m.\u001B[39mshow(image, title\u001B[38;5;241m=\u001B[39mtitle, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions):\n\u001B[0;32m     63\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\PIL\\ImageShow.py:86\u001B[0m, in \u001B[0;36mViewer.show\u001B[1;34m(self, image, **options)\u001B[0m\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m image\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m!=\u001B[39m base:\n\u001B[0;32m     84\u001B[0m         image \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mconvert(base)\n\u001B[1;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshow_image(image, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\PIL\\ImageShow.py:112\u001B[0m, in \u001B[0;36mViewer.show_image\u001B[1;34m(self, image, **options)\u001B[0m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow_image\u001B[39m(\u001B[38;5;28mself\u001B[39m, image, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions):\n\u001B[0;32m    111\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Display the given image.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 112\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshow_file(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_image(image), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\PIL\\ImageShow.py:129\u001B[0m, in \u001B[0;36mViewer.show_file\u001B[1;34m(self, path, **options)\u001B[0m\n\u001B[0;32m    127\u001B[0m         msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    128\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 129\u001B[0m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msystem\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# nosec\u001B[39;00m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "loss = train_bpgm(dataloader=dataloader,\n",
    "                  model=model, device=device, epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:33:10.833093900Z",
     "start_time": "2023-06-01T12:17:11.857844800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_model(model.to('cpu'), path=f\"{save_directory}\\\\epoch_02.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:33:10.897222300Z",
     "start_time": "2023-06-01T12:33:10.834097600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# тест"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "testtransform = transforms.Compose([\n",
    "    transforms.Resize((256,192)),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "testmask_transform = transforms.Compose([\n",
    "    transforms.Resize((256,192)),\n",
    "   ])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "test_dataset = ShirtsDataset(\n",
    "    root=r\"C:\\Users\\Даша\\кто\\мусор\\zalando-hd-resized\\test\",\n",
    "    transform=testtransform,\n",
    "    transform_mask=testmask_transform\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:10:15.686014600Z",
     "start_time": "2023-05-31T19:10:15.670314500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "topil = transforms.ToPILImage()\n",
    "toten = transforms.ToTensor()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:33:10.899626500Z",
     "start_time": "2023-06-01T12:33:10.897222300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "testdataloader = DataLoader(test_dataset, batch_size=1, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:10:18.664984300Z",
     "start_time": "2023-05-31T19:10:18.635883100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "weights_dir = r'C:\\Users\\Даша\\кто\\мусор\\epoch_02.pt'\n",
    "#r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\tps\\weights\\test\\epoch_00.pt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T10:33:06.662842800Z",
     "start_time": "2023-06-01T10:33:06.660835700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "model = load_model(model, weights_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T10:33:07.259770400Z",
     "start_time": "2023-06-01T10:33:07.211677800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "count = 0\n",
    "for data in testdataloader:\n",
    "    mask = data['segmentation'].to(device)\n",
    "    shirt = data['shirt'].to(device)\n",
    "    shirt_mask = data['shirt_mask'].to(device)\n",
    "\n",
    "    model = model.to(device)\n",
    "    theta = model(mask, shirt)\n",
    "\n",
    "    warped = F.grid_sample(shirt, theta, padding_mode='border', align_corners=True).to('cpu')\n",
    "\n",
    "    warped_mask = F.grid_sample(shirt_mask, theta, padding_mode='border', align_corners=True).to('cpu')\n",
    "    warped_mask = topil(warped_mask[0])\n",
    "    warped_mask.show()\n",
    "    warped = topil(warped[0]/2+0.5)\n",
    "    warped.show()\n",
    "    count += 1\n",
    "    if count == 4:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-31T19:20:02.683557500Z",
     "start_time": "2023-05-31T19:19:37.118697300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "подготовка"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "path_people = r'C:\\Users\\Даша\\кто\\мусор\\zalando-hd-resized\\test\\imageWithNoCloth'\n",
    "#r'C:\\Users\\DenisovDmitrii\\Desktop\\forEncoderNew\\train\\imageWithNoCloth'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:33:10.906372700Z",
     "start_time": "2023-06-01T12:33:10.899626500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "cloud_people = os.listdir(path_people)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:33:10.921412900Z",
     "start_time": "2023-06-01T12:33:10.906372700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "2032"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cloud_people)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:33:10.923714600Z",
     "start_time": "2023-06-01T12:33:10.921412900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Даша\\AppData\\Local\\Temp\\ipykernel_4228\\3884460301.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cwm = torch.tensor(cwm, dtype=torch.bool)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 15\u001B[0m\n\u001B[0;32m     12\u001B[0m cwm \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(cwm, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mbool)\n\u001B[0;32m     14\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 15\u001B[0m theta \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshirt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m warped \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mgrid_sample(shirt, theta, padding_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mborder\u001B[39m\u001B[38;5;124m'\u001B[39m, align_corners\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     18\u001B[0m warped \u001B[38;5;241m=\u001B[39m warped \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0.5\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\LookGenerator\\networks\\bpgm\\model\\models.py:287\u001B[0m, in \u001B[0;36mBPGM.forward\u001B[1;34m(self, inputA, inputB)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputA, inputB):\n\u001B[0;32m    286\u001B[0m     featureA \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mextractionA(inputA)\n\u001B[1;32m--> 287\u001B[0m     featureB \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextractionB\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputB\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    289\u001B[0m     featureA \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ml2norm(featureA)\n\u001B[0;32m    290\u001B[0m     featureB \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ml2norm(featureB)\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\LookGenerator\\networks\\bpgm\\model\\models.py:34\u001B[0m, in \u001B[0;36mFeatureExtraction.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 34\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 204\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\SMBackEnd\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# ИСПОЛЬЗОВАТЬ batch_size=1\n",
    "count = 0\n",
    "for index, data in enumerate(testdataloader):\n",
    "\n",
    "    cloud_person = Image.open(path_people + \"\\\\\" + cloud_people[index])\n",
    "    cloud_person = toten(cloud_person)\n",
    "\n",
    "    mask = data['segmentation'].to(device)\n",
    "    shirt = data['shirt'].to(device)\n",
    "    cwm = data['torso'].to(device)\n",
    "\n",
    "    cwm = torch.tensor(cwm, dtype=torch.bool)\n",
    "\n",
    "    model = model.to(device)\n",
    "    theta = model(mask, shirt)\n",
    "\n",
    "    warped = F.grid_sample(shirt, theta, padding_mode='border', align_corners=True).to('cpu')\n",
    "    warped = warped / 2 + 0.5\n",
    "    warped = warped * cwm\n",
    "\n",
    "    cwm = ~cwm\n",
    "\n",
    "    person = cloud_person * cwm + warped\n",
    "    # wc = transforms.Resize((256 + 256 // 2, 192 + 192 // 2))(wc)\n",
    "    # wc = transforms.CenterCrop((256, 192))(wc)\n",
    "    #warped_mask = np.array(topil(warped_mask[0]))\n",
    "    # warped = np.array(topil(warped[0]))\n",
    "    #wc = np.array(topil(wc[0]))\n",
    "\n",
    "    person = topil(person[0]).convert('RGB')\n",
    "\n",
    "    person.save(path_people + \"2\\\\\" + cloud_people[index])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-01T12:33:29.288322100Z",
     "start_time": "2023-06-01T12:33:10.924711Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
