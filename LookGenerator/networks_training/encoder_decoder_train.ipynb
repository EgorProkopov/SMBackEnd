{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Импорт"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import albumentations\n",
    "from LookGenerator.networks.losses import PerceptualLoss, PerPixelLoss\n",
    "from LookGenerator.datasets.encoder_decoder_datasets import EncoderDecoderDataset\n",
    "from LookGenerator.networks.trainer import Trainer\n",
    "from LookGenerator.networks.clothes_feature_extractor import ClothAutoencoder\n",
    "from LookGenerator.networks.encoder_decoder import EncoderDecoder\n",
    "from LookGenerator.networks_training.utils import check_path_and_creat\n",
    "import LookGenerator.datasets.transforms as custom_transforms\n",
    "from LookGenerator.networks.utils import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Загрузка данных"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "transform_human = transforms.Compose([\n",
    "    transforms.Resize((256, 192)) #,\n",
    "    # transforms.RandomAffine(scale=(0.8, 1), degrees=(-90,90), fill = 0.9),\n",
    "    # transforms.ColorJitter(brightness=(0.5, 1), contrast=(0.4,1),  hue=(0, 0.3)),\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "    #                      std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "transform_pose_points=transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    custom_transforms.MinMaxScale()\n",
    "])\n",
    "\n",
    "transform_clothes = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    # transforms.ColorJitter(brightness=(0.5, 1), contrast=(0.4,1),  hue=(0, 0.3)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "transform_human_restored = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    # transforms.RandomAffine(scale=(0.8, 1), degrees=(-90,90), fill = 0.9),\n",
    "    # transforms.ColorJitter(brightness=(0.5, 1), contrast=(0.4,1),  hue=(0, 0.3)),\n",
    "    custom_transforms.MinMaxScale()\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "batch_size_train = 32\n",
    "batch_size_val = 16\n",
    "pin_memory = True\n",
    "num_workers = 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_dataset = EncoderDecoderDataset(\n",
    "    image_dir=r\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoder\\train\",\n",
    "    transform_human=transform_human,\n",
    "    transform_clothes=transform_clothes,\n",
    "    transform_human_restored=transform_human_restored\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size_train, shuffle=True, pin_memory=pin_memory, num_workers=num_workers\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T20:21:16.756719500Z",
     "start_time": "2023-05-30T20:21:16.738368800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "val_dataset = EncoderDecoderDataset(\n",
    "    image_dir=r\"C:\\Users\\DenisovDmitrii\\Desktop\\forEncoder\\val\",\n",
    "    transform_human=transform_human,\n",
    "    transform_clothes=transform_clothes,\n",
    "    transform_human_restored=transform_human_restored,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size_val, shuffle=False, pin_memory=pin_memory, num_workers=num_workers\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T20:21:16.761625100Z",
     "start_time": "2023-05-30T20:21:16.756719500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 256, 192])\n",
      "torch.Size([32, 3, 256, 192])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T20:21:24.218794100Z",
     "start_time": "2023-05-30T20:21:16.761625100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Лосс"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class EncoderDecoderLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder-decoder custom loss\n",
    "    \"\"\"\n",
    "    def __init__(self, device='cpu'):\n",
    "        super(EncoderDecoderLoss, self).__init__()\n",
    "        self.perceptual_loss = PerceptualLoss(device, weights_perceptual=[1.0, 1.0, 1.0, 1.0])\n",
    "        self.per_pixel_loss = PerPixelLoss().to(device)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss = self.perceptual_loss(outputs, targets) + self.per_pixel_loss(outputs, targets)\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T20:21:24.221893200Z",
     "start_time": "2023-05-30T20:21:24.218794100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Обучение модели"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "clothes_feature_extractor = ClothAutoencoder(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    features=(8, 16, 32, 64),\n",
    "    latent_dim_size=128,\n",
    "    encoder_activation_func=nn.LeakyReLU(),\n",
    "    decoder_activation_func=nn.ReLU()\n",
    ")\n",
    "clothes_feature_extractor = load_model(clothes_feature_extractor, r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\autoDegradation\\weights\\testClothes_L1Loss_4features\\epoch_39.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T20:21:24.244234100Z",
     "start_time": "2023-05-30T20:21:24.221893200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DenisovDmitrii\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DenisovDmitrii\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoder(clothes_feature_extractor, in_channels=6, out_channels=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "criterion = EncoderDecoderLoss(device=device)\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T20:21:24.852767900Z",
     "start_time": "2023-05-30T20:21:24.243230700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory=r\"C:\\Users\\DenisovDmitrii\\OneDrive - ITMO UNIVERSITY\\peopleDetector\\newEncoder\\weights\\testBaseParams\"\n",
    "check_path_and_creat(save_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T20:21:24.852767900Z",
     "start_time": "2023-05-30T20:21:24.852767900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T20:21:24.852767900Z",
     "start_time": "2023-05-30T20:21:24.852767900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model_=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    save_directory=save_directory,\n",
    "    save_step=1,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T20:21:24.857720500Z",
     "start_time": "2023-05-30T20:21:24.852767900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time 30-05-2023 23:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/364 [00:00<?, ?it/s]C:\\Users\\DenisovDmitrii\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 47/364 [01:19<08:58,  1.70s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch_num\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\python\\SMBackEnd\\LookGenerator\\networks\\trainer.py:57\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, train_dataloader, val_dataloader, epoch_num)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstart time\u001B[39m\u001B[38;5;124m\"\u001B[39m, start\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch_num):\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[1;32m---> 57\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_history_epochs\u001B[38;5;241m.\u001B[39mappend(train_loss)\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose:\n",
      "File \u001B[1;32mC:\\python\\SMBackEnd\\LookGenerator\\networks\\trainer.py:109\u001B[0m, in \u001B[0;36mTrainer._train_epoch\u001B[1;34m(self, train_dataloader)\u001B[0m\n\u001B[0;32m    107\u001B[0m     loss_number \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m    108\u001B[0m     train_running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss_number\n\u001B[1;32m--> 109\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_history_batches\u001B[38;5;241m.\u001B[39mappend(loss_number)\n\u001B[0;32m    111\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m train_running_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_dataloader)\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m train_loss\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(train_dataloader, val_dataloader, epoch_num=20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-30T20:22:45.240447100Z",
     "start_time": "2023-05-30T20:21:24.858716900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.draw_history_plots()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T20:19:20.078351Z",
     "end_time": "2023-04-03T20:19:20.589910Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train(train_dataloader, val_dataloader, epoch_num=20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T20:24:07.630751Z",
     "end_time": "2023-04-03T21:38:46.478628Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.draw_history_plots()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T21:38:46.481638Z",
     "end_time": "2023-04-03T21:38:46.575465Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
