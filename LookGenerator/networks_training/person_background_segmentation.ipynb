{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "\n",
    "import albumentations\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from LookGenerator.datasets.utils import load_image, prepare_image_for_segmentation, to_array_from_model_bin, show_array_multichannel\n",
    "from LookGenerator.datasets.background_cut_dataset import PersonDataset\n",
    "from LookGenerator.networks.losses import FocalLoss\n",
    "from LookGenerator.networks.segmentation import UNet, train_unet\n",
    "from LookGenerator.networks.utils import load_model\n",
    "import LookGenerator.datasets.transforms as custom_transforms\n",
    "from LookGenerator.networks_training.utils import check_path_and_creat\n",
    "from LookGenerator.config.config import DatasetConfig\n",
    "from LookGenerator.networks.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform_input = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.25, 0.25, 0.25]\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_output = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    custom_transforms.MinMaxScale(),\n",
    "    custom_transforms.ThresholdTransform(threshold=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size_train = 24\n",
    "batch_size_val = 16\n",
    "pin_memory = True\n",
    "num_workers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "transform_train = albumentations.Compose([\n",
    "        albumentations.Resize(height=256, width=192),\n",
    "        albumentations.RandomBrightnessContrast(brightness_limit=(0.1,0.3), contrast_limit=(0.2,0.7), p =0.2),\n",
    "        albumentations.Equalize(p = 0.2),\n",
    "        albumentations.GaussNoise(p = 0.2),\n",
    "        albumentations.Affine(translate_percent=0.1, scale=(0.8, 1), rotate=(-90,90), p=0.2),\n",
    "        albumentations.Normalize(mean = (0.5, 0.5, 0.5), std = (0.25, 0.25, 0.25)),\n",
    "    ])\n",
    "\n",
    "transform_valid = albumentations.Compose([\n",
    "        albumentations.Resize(height=256, width=192),\n",
    "        albumentations.Normalize(mean = (0.5, 0.5, 0.5), std = (0.25, 0.25, 0.25)),\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Config = DatasetConfig(os.environ)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(11647, 486)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = PersonDataset(\n",
    "    Config.DATASET_DIR, \"image\", \"image-parse-v3\",\n",
    "    background_root_dir=r\"\", dir_name_background=\"ScreenShot\",\n",
    "    transform_input=transform_input, transform_output=transform_output, augment=transform_train\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True, pin_memory=pin_memory, num_workers=num_workers)\n",
    "(len(train_dataset), len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(2032, 127)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = PersonDataset(\n",
    "    Config.DATASET_DIR, \"image\", \"image-parse-v3\",\n",
    "    background_root_dir=r\"\", dir_name_background=\"ScreenShot\",\n",
    "    transform_input=transform_input, transform_output=transform_output, augment=transform_train\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=False, pin_memory=pin_memory, num_workers=num_workers)\n",
    "(len(val_dataset), len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    plt.imshow(X.detach().numpy()[0,0,:,:], cmap = 'binary')\n",
    "    plt.show()\n",
    "    show_array_multichannel(y.detach().numpy()[0,:, :, :], 15)\n",
    "\n",
    "    # modelled_img = to_array_from_model_bin_transpose(transform_output(X.detach()))\n",
    "    # plt.imshow(modelled_img,cmap = 'binary')\n",
    "    # plt.show()\n",
    "    # modelled_img = to_array_from_model_bin_transpose(transform_output(y.detach()))\n",
    "    # plt.imshow(modelled_img,cmap = 'binary')\n",
    "    # plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model = UNet(in_channels=3, out_channels = 1)\n",
    "#model = load_model(model, r'')                 # активировать, если модель надо загрузить и дообучить\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "criterion = FocalLoss()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# torch.backends.cuda.matmul.allow_tf32 = False\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "# import datetime\n",
    "# now = datetime.datetime.now()\n",
    "# print(\"start time\", now.strftime(\"%d-%m-%Y %H:%M\"))\n",
    "#TODO: указать директорию для сохранения весов\n",
    "# save_directory = \"\"\n",
    "# check_path_and_creat(save_directory)\n",
    "# train_history, val_history = train_unet(\n",
    "#     model,\n",
    "#     train_dataloader,\n",
    "#     val_dataloader,\n",
    "#     optimizer,\n",
    "#     device=device,\n",
    "#     epoch_num=20,\n",
    "#     save_directory= save_directory\n",
    "# )\n",
    "# old = now\n",
    "# now = datetime.datetime.now()\n",
    "# print(\"end time\", now.strftime(\"%d-%m-%Y %H:%M\"))\n",
    "# print(\"delta\", now - old)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_directory = \"\"\n",
    "epoch_num = 30"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model_=model, optimizer=optimizer, criterion = criterion,\n",
    "    device=device, save_directory=save_directory, save_step=2, verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train(train_dataloader=train_dataloader, val_dataloader=val_dataloader, epoch_num=epoch_num)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.draw_history_plots()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Загрузка модели и проверка визуально качества работы"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = load_model(model, r'C:\\Users\\Даша\\PycharmProjects\\SMBackEnd\\LookGenerator\\weights\\unet_epoch_0_0.0161572862694324.pt')\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dir = r\"\"\n",
    "test_folder = \"\"\n",
    "save_masks_dir = r\"\"\n",
    "list_files = os.listdir(test_dir)\n",
    "images = [file.split('.') for file in list_files]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    print(image)\n",
    "    img = load_image(test_dir, test_folder, image, '.jpg')\n",
    "    img_to_model = prepare_image_for_segmentation(img, transform_input)\n",
    "    modelled = model(img_to_model)\n",
    "    mask = to_array_from_model_bin(modelled)\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(1,2,0)\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(mask)\n",
    "\n",
    "    Image.fromarray(mask, 'L').save(save_masks_dir + image + '.png')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
